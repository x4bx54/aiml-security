# AIML Threats & Mitigations
This page will cover the common threats and mitigation recommendations that can better aid in the secure design of your AIML system. 

![Threat_Overview](https://github.com/user-attachments/assets/9ea4a8ae-142f-4020-8abc-e2b0cdfb854f)

From an secure design perpsective, it is best to apply a zero-trust principle... check all input and output from the LLM (ironically, you have to treat it as untrusted no matter what)

### 1. Poisoned Model & Poor Model Selection


### 2. Data Poisoning
Data poisoning applies to the data used for training the model or fine-tuning the model

### 3. Prompt Injection, Hallucinations, Model Denial-of-Service, Denial-of-Wallet

### 4. Output Handling

### 5. Excessive Agency 

## Useful Resources
+ https://atlas.mitre.org/
+ https://owasp.org/www-project-top-10-for-large-language-model-applications/
+ https://owasp.org/www-project-machine-learning-security-top-10/
  
